{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Cloud Price Comparison\n",
    "How do cloud providers stack up?\n",
    " \n",
    "Making apples to apples comparisons between different cloud providers is very difficult, because each one offers instances with varying vCPUs, RAM, SSD space and HDD space. To further obfuscate matters, slightly different billing systems, promises of arcane discounting, only providing pricing in USD, and inconsistent naming conventions are sprinkled throughout.\n",
    "\n",
    "As an attempt to provide a clearer price comparison, I'll be using [multiple linear regression](https://en.wikipedia.org/wiki/Linear_regression) to \"[normalise](http://bit.ly/2xIUM5C)\" the pricing of compute instances across different cloud providers.\n",
    "\n",
    "In essence, **If every cloud provider offered the same size compute instances, how expensive would they be?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Todo\n",
    "\n",
    "* Account for GST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dataset\n",
    "\n",
    "I'll be taking the price tables of:\n",
    "* Google Cloud - [Predefined machine types](https://cloud.google.com/compute/pricing#predefined_machine_types)\n",
    "* AWS - [On demand instances](https://aws.amazon.com/ec2/pricing/on-demand/)\n",
    "* Azure - [Linux virtual machines](https://azure.microsoft.com/en-us/pricing/details/virtual-machines/linux/)\n",
    "\n",
    "and converting them into the instance sizes offered by [Catalyst Cloud](https://www.catalyst.net.nz/catalyst-cloud/prices). You can find the datasets and their sources [here](https://github.com/catalyst-cloud/catalystcloud-price-comparison/raw/master/dataset/Cloud%20price%20comparison.ods)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The list of URLS\n",
    "\n",
    "# Catalyst Cloud's price list\n",
    "catalyst_url = 'https://catalystcloud.nz/services/iaas/compute'\n",
    "catalyst_data_location = 'dataset/catalyst_price_data.csv'\n",
    "\n",
    "# AWS's bulk price list JSON, for the Sydney region (ap-southeast-2)\n",
    "aws_url = 'https://aws.amazon.com/ec2/pricing/on-demand/'\n",
    "aws_raw_location = 'dataset/raw_aws_bulk.json'\n",
    "aws_acceptable_instance_families = [\n",
    "    'General purpose',\n",
    "    'Memory optimized',\n",
    "    'Micro instances'\n",
    "]\n",
    "aws_data_location = 'dataset/aws_price_data.csv'\n",
    "\n",
    "# Google Cloud's prices\n",
    "google_url = 'https://cloud.google.com/compute/pricing'\n",
    "google_price_type = 'syd-hourly'\n",
    "google_acceptable_instance_families = [\n",
    "    'standard_machine_types',\n",
    "    'high-memory_machine_types',\n",
    "    'high-cpu_machine_types',\n",
    "    \n",
    "]\n",
    "google_data_location = 'dataset/google_price_data.csv'\n",
    "\n",
    "# Azure\n",
    "azure_url = 'http://www.azureinstances.info/api/Compute/Get?delimitedRegions=Australia Southeast'\n",
    "azure_raw_location = 'dataset/raw_azure_bulk.json'\n",
    "azure_data_location = 'dataset/azure_price_data.csv'\n",
    "\n",
    "# List of existing dataset files\n",
    "datasets = os.listdir('dataset')\n",
    "\n",
    "fresh = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exchange rate\n",
    "Getting the USD to NZD exchange rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "usd_to_nzd_exchange_rate_url = 'http://free.currencyconverterapi.com/api/v5/convert?q=USD_NZD&compact=y'\n",
    "\n",
    "usd_to_nzd_exchange_rate_json = requests.get(usd_to_nzd_exchange_rate_url).json()\n",
    "usd_to_nzd_exchange_rate = float(usd_to_nzd_exchange_rate_json['USD_NZD']['val'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catalyst Cloud prices\n",
    "Scraping the Catalyst Cloud compute page for prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalyst_price_page_html = requests.get(catalyst_url).text \n",
    "catalyst_price_page = BeautifulSoup(catalyst_price_page_html, 'html.parser')\n",
    "\n",
    "catalyst_price_table = catalyst_price_page.find(attrs={'class': 'service-price-table'}).tbody\n",
    "catalyst_price_rows = catalyst_price_table.find_all('tr')\n",
    "\n",
    "catalyst_prices_list = []\n",
    "\n",
    "for row in catalyst_price_rows:\n",
    "    catalyst_price_cells = list(row.stripped_strings)\n",
    "    catalyst_prices_list.append({\n",
    "        'Name': catalyst_price_cells[0],\n",
    "        'vCPU': float(catalyst_price_cells[1]),\n",
    "        'RAM, GB': float(catalyst_price_cells[2]),\n",
    "        'Price per hour, NZD (ex GST)': float(catalyst_price_cells[3].strip('$')),\n",
    "        'SSD storage, GB': .0,\n",
    "        'HDD storage, GB': .0\n",
    "    })\n",
    "    \n",
    "pd.DataFrame(catalyst_prices_list).to_csv(catalyst_data_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWS prices\n",
    "\n",
    "Scraping the AWS on demand pricing page for prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWS raw dataset already dowloaded.\n"
     ]
    }
   ],
   "source": [
    "if aws_raw_location.split('/')[1] not in datasets or fresh:\n",
    "    aws_bulk_json_requeset = requests.get(aws_url)\n",
    "    aws_bulk_json = aws_bulk_json_requeset.json()\n",
    "    with open(aws_raw_location, 'w') as aws_raw_file:\n",
    "        json.dump(aws_bulk_json, aws_raw_file)\n",
    "    print('Downloaded most recent AWS price list.')\n",
    "else:\n",
    "    print('AWS raw dataset already dowloaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(aws_raw_location, 'r') as aws_raw_file:\n",
    "    aws_raw_json = json.load(aws_raw_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the specific relevant prices from the raw AWS file, and putting them in a consistant, usable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the instance products\n",
    "\n",
    "with open(aws_raw_location, 'r') as aws_raw_file:\n",
    "    aws_raw_json = json.load(aws_raw_file)\n",
    "        \n",
    "    aws_instances_list = []\n",
    "            \n",
    "    for product in aws_raw_json['products']:\n",
    "        \n",
    "        productFamily = aws_raw_json['products'][product]['productFamily']\n",
    "        \n",
    "        # Check product is compute instance\n",
    "        if productFamily == 'Compute Instance':\n",
    "                        \n",
    "            # Check if instance is appropriate\n",
    "            instanceFamily = aws_raw_json['products'][product]['attributes']['instanceFamily']\n",
    "            is_current_gen = aws_raw_json['products'][product]['attributes']['currentGeneration'] == 'Yes'\n",
    "            is_linux = aws_raw_json['products'][product]['attributes']['operatingSystem'] == 'Linux'\n",
    "            no_preInstalledSw = aws_raw_json['products'][product]['attributes']['preInstalledSw'] == 'NA'\n",
    "            is_shared_instance = aws_raw_json['products'][product]['attributes']['tenancy'] == 'Shared'\n",
    "\n",
    "            if instanceFamily in aws_acceptable_instance_families and is_current_gen \\\n",
    "                and is_linux and no_preInstalledSw and is_shared_instance:\n",
    "                \n",
    "                # Append if appropriate\n",
    "                aws_instances_list.append(product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with open(aws_raw_location, 'r') as aws_raw_file:\n",
    "    \n",
    "    aws_prices_list = []\n",
    "    \n",
    "    for instance_key in aws_instances_list:\n",
    "\n",
    "        attributes = aws_raw_json['products'][instance_key]['attributes']\n",
    "                    \n",
    "        # Get vCPU and RAM\n",
    "        vCPU = float(attributes['vcpu'].replace(',',''))\n",
    "        RAM = float(attributes['memory'].split(' ')[0].replace(',',''))\n",
    "\n",
    "        # Break storage spec into array\n",
    "        storage_strings = attributes['storage'].split(' ')\n",
    "\n",
    "        # Find where the numbers end (200 x 1), and the description of the storage type (SSD) starts.\n",
    "        final_num_index = None\n",
    "        for word in storage_strings[::-1]:\n",
    "            try:\n",
    "                float(word.replace(',', ''))\n",
    "                final_num_index = storage_strings.index(word)\n",
    "                break\n",
    "            except:\n",
    "                foo = None\n",
    "\n",
    "        # If there are no numbers in the storage spec, there is no storage included\n",
    "        if final_num_index == None:\n",
    "\n",
    "            total_ssd = .0\n",
    "            total_hdd = .0\n",
    "\n",
    "        # Else...\n",
    "        else:\n",
    "\n",
    "            # Perform the math to figure out how many GB of storage is included\n",
    "            storage_calcs = storage_strings[0:final_num_index+1]\n",
    "            storage_volume = eval(' '.join(['*' if x=='x' else x.replace(',', '') for x in storage_calcs]))\n",
    "\n",
    "            # discern the type of storage\n",
    "            if 'HDD' in storage_strings:                        \n",
    "                total_ssd = .0\n",
    "                total_hdd = float(storage_volume)\n",
    "\n",
    "            elif 'SSD' in storage_strings:                        \n",
    "                total_ssd = float(storage_volume)\n",
    "                total_hdd = .0\n",
    "            else: \n",
    "                total_ssd = float(storage_volume)\n",
    "                total_hdd = .0\n",
    "\n",
    "\n",
    "        # Get the price per USD\n",
    "        terms = aws_raw_json['terms']['OnDemand'][instance_key]\n",
    "        usd_price = None\n",
    "        for specific_term in terms:\n",
    "            for dimension_key in terms[specific_term]['priceDimensions']:\n",
    "                dimension = terms[specific_term]['priceDimensions'][dimension_key]\n",
    "                if dimension['unit'] != 'Hrs': raise ValueError(\"This price isn't in hours\")\n",
    "                usd_price = float(dimension['pricePerUnit']['USD'])\n",
    "\n",
    "        # Convert to NZD\n",
    "        nzd_price = usd_price * usd_to_nzd_exchange_rate\n",
    "                \n",
    "        # Append to list of prices\n",
    "        aws_prices_list.append({\n",
    "            'Name': attributes['instanceType'],\n",
    "            'vCPU': vCPU,\n",
    "            'RAM, GB': RAM,\n",
    "            'Price per hour, NZD (ex GST)': nzd_price,\n",
    "            'SSD storage, GB': total_ssd,\n",
    "            'HDD storage, GB': total_hdd\n",
    "        })\n",
    "\n",
    "# Convert to CSV\n",
    "pd.DataFrame(aws_prices_list).to_csv(aws_data_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Cloud prices\n",
    "\n",
    "Scraping Google Cloud's documentation for prices of custom instance sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_price_page_html = requests.get(google_url).text\n",
    "google_price_page = BeautifulSoup(google_price_page_html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the USD price per vCPU and per GB RAM\n",
    "google_custom_compute_price_table = google_price_page.find(id='custommachinetypepricing').find_next('table')\n",
    "google_custom_compute_rows = google_custom_compute_price_table.find_all('tr')[1:]\n",
    "\n",
    "google_per_vcpu_usd = float(google_custom_compute_rows[0].find_all('td')[1][google_price_type].split()[0].strip('$'))\n",
    "google_per_ram_usd = float(google_custom_compute_rows[1].find_all('td')[1][google_price_type].split()[0].strip('$'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_freq_num(text):\n",
    "    number_list = re.findall('\\d*\\.?\\d+', text)\n",
    "    most_frequent_num = max(set(number_list), key=number_list.count)\n",
    "    return float(most_frequent_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "google_prices_list = []\n",
    "\n",
    "for instance_type in google_acceptable_instance_families:\n",
    "    \n",
    "    google_price_table = google_price_page.find(id=instance_type).find_next('table')\n",
    "    google_rows = google_price_table.find_all('tr')[1:-1]\n",
    "        \n",
    "    for row in google_rows:\n",
    "        \n",
    "        # Extract number of vCPUs and GB of RAM\n",
    "        try:\n",
    "            cells = row.find_all('td')\n",
    "            name = cells[0].get_text().strip()\n",
    "            # Ignore if has lake in name (to remove skylake instances)\n",
    "            if 'lake' in name:\n",
    "                continue\n",
    "            cpu_val = most_freq_num(str(cells[1]))\n",
    "            ram_val = most_freq_num(str(cells[2]))\n",
    "        except:\n",
    "            foo='bar'\n",
    "                        \n",
    "        # Calcluate NZD price\n",
    "        usd_price = (google_per_ram_usd * ram_val) + (google_per_vcpu_usd * cpu_val)\n",
    "        nzd_price = usd_price * usd_to_nzd_exchange_rate\n",
    "        \n",
    "        try:\n",
    "            google_prices_list.append({\n",
    "                'Name': name,\n",
    "                'vCPU': cpu_val,\n",
    "                'RAM, GB': ram_val,\n",
    "                'Price per hour, NZD (ex GST)': usd_price,\n",
    "                'SSD storage, GB': .0,\n",
    "                'HDD storage, GB': .0\n",
    "            })\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "pd.DataFrame(google_prices_list).to_csv(google_data_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Azure cloud prices\n",
    "Get Azure pricing from community maintained price list. This is because it is too difficult to scrape the Azure pricing page, and they maintain no APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure raw dataset already dowloaded.\n"
     ]
    }
   ],
   "source": [
    "# azure_price_xml =  requests.get(azure_url).content\n",
    "\n",
    "if azure_raw_location.split('/')[1] not in datasets or fresh:\n",
    "    azure_bulk_json_requeset = requests.get(azure_url)\n",
    "    azure_bulk_json = azure_bulk_json_requeset.json()\n",
    "    with open(azure_raw_location, 'w') as azure_raw_file:\n",
    "        json.dump(azure_bulk_json, azure_raw_file)\n",
    "    print('Downloaded most recent Azure price list.')\n",
    "else:\n",
    "    print('Azure raw dataset already dowloaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingesting the datasets\n",
    "Importing the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalyst_dataset = pd.read_csv(\"dataset/Rigorious Cloud price comparison - Exportable Catalyst prices.csv\")\n",
    "google_dataset = pd.read_csv(\"dataset/Rigorious Cloud price comparison - Exportable Google prices.csv\")\n",
    "aws_dataset = pd.read_csv(\"dataset/Rigorious Cloud price comparison - Exportable AWS prices.csv\")\n",
    "azure_dataset = pd.read_csv(\"dataset/Rigorious Cloud price comparison - Exportable Azure prices.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previewing the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalyst_dataset.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll split the data into NumPy arrays of input features (X) and labels (Y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset (dataset):\n",
    "    x = dataset[[\"vCPUs\", \"RAM (GiB)\", \"HDD (GB)\", \"SSD (GB)\"]].values\n",
    "    y = dataset[\"Price per hour (NZD)\"].values\n",
    "    \n",
    "    return (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalyst_x, catalyst_y = split_dataset(catalyst_dataset)\n",
    "google_x, google_y = split_dataset(google_dataset)\n",
    "aws_x, aws_y = split_dataset(aws_dataset)\n",
    "azure_x, azure_y = split_dataset(azure_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The math\n",
    "To analyse this dataset, we'll be using [multiple linear regression](https://en.wikipedia.org/wiki/Linear_regression) to predict the prices of compute instance flavors if they were being offered by cloud providers that do not typically offer that sized flavor.\n",
    "\n",
    "The multiple linear regression models will draw a hyperplane across our data space (in this case, 5 dimensional space) that comes as close as possible to intersecting every data point in our dataset. You can see an example of this (in 2 dimensional space) by Khan academy below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML('<iframe width=\"840\" height=\"472\" src=\"https://www.youtube-nocookie.com/embed/GAmzwIkGFgE?rel=0\" frameborder=\"0\" allowfullscreen></iframe>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By then finding a point on the hyperplane that intersects with our vCPUs, RAM, HDD, and SSD axes, we can find the predicted price. This will give us a way to estimate the price of a flavour if it were offered by various cloud providers, even if they do not offer it.\n",
    "\n",
    "I've used linear regression as the predictive algorithm because I'm assuming that cloud providers scale their pricing in a linear patern. For example, Catalyst Cloud decides the price of their flavors with the equation: \n",
    "\n",
    "Price/hour = (RAM gb x 0.18) + (vCPU x 0.026)\n",
    "\n",
    "First we'll initialise the regression models, and train them on the cloud providers' prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise regressors\n",
    "catalyst_linear = LinearRegression()\n",
    "google_linear = LinearRegression()\n",
    "aws_linear = LinearRegression()\n",
    "azure_linear = LinearRegression()\n",
    "\n",
    "# Train regressors\n",
    "catalyst_linear.fit(catalyst_x, catalyst_y)\n",
    "google_linear.fit(google_x, google_y)\n",
    "aws_linear.fit(aws_x, aws_y)\n",
    "azure_linear.fit(azure_x, azure_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the models predict the other providers' instance prices. By having all providers predict the prices of all other providers, we can see if the pattern is maintained across different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict Catalyst X\n",
    "google_cata_price = google_linear.predict(catalyst_x)\n",
    "aws_cata_price = aws_linear.predict(catalyst_x)\n",
    "azure_cata_price = azure_linear.predict(catalyst_x)\n",
    "\n",
    "# Predict Google X\n",
    "aws_goog_price = aws_linear.predict(google_x)\n",
    "azure_goog_price = azure_linear.predict(google_x)\n",
    "catalyst_goog_price = catalyst_linear.predict(google_x)\n",
    "\n",
    "# Predict AWS X\n",
    "google_aws_price = google_linear.predict(aws_x)\n",
    "azure_aws_price = azure_linear.predict(aws_x)\n",
    "catalyst_aws_price = catalyst_linear.predict(aws_x)\n",
    "\n",
    "# Predict Azure X\n",
    "google_azure_price = google_linear.predict(azure_x)\n",
    "aws_azure_price = aws_linear.predict(azure_x)\n",
    "catalyst_azure_price = catalyst_linear.predict(azure_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The results\n",
    "\n",
    "Now we have the results together, where we can compare the prices against each other on an even scale.\n",
    "\n",
    "A good scientist would, at this point, verify their results by comparing an intersection between the predicted output and the actual output. I would love to do this. However I could find no such intersection.\n",
    "\n",
    "You can find the datasets this analysis is based on [here](https://github.com/catalyst-cloud/catalystcloud-price-comparison/raw/master/dataset/Cloud%20price%20comparison.ods), and a chart plotting this data, [here](https://object-storage.nz-por-1.catalystcloud.io/v1/AUTH_8ccc3286887e49cb9a40f023eba693b4/catalyst-cloud-price-comp/).\n",
    "\n",
    "Please note that the X axis is a range from zero to the number of flavors offered by each provider. Each number on the X axis represents a single flavor by the provider we are predicting. I've done it this way because the plotting method does not support non-numerical axis ticks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_pred (names, predictions):\n",
    "    flavors_num = predictions[0].shape[0]\n",
    "    for index, name in enumerate(names):\n",
    "        plt.plot(range(flavors_num), predictions[index], label=names[index])\n",
    "    plt.legend(loc=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Catalyst Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_pred([\n",
    "        \"Catalyst\", \"Google\", \"AWS\", \"Azure\"\n",
    "    ], [\n",
    "        catalyst_y, google_cata_price, aws_cata_price, azure_cata_price\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Google Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_pred([\n",
    "        \"Catalyst\", \"Google\", \"AWS\", \"Azure\"\n",
    "    ], [\n",
    "        catalyst_goog_price,\n",
    "        google_y,\n",
    "        aws_goog_price,\n",
    "        azure_goog_price,\n",
    "        \n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_pred([\n",
    "        \"Catalyst\", \"Google\", \"AWS\", \"Azure\"\n",
    "    ], [\n",
    "        catalyst_aws_price,\n",
    "        google_aws_price,\n",
    "        aws_y,\n",
    "        azure_aws_price,\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_pred([\n",
    "        \"Catalyst\", \"Google\", \"AWS\", \"Azure\"\n",
    "    ], [\n",
    "        catalyst_azure_price,\n",
    "        google_azure_price,\n",
    "        aws_azure_price,\n",
    "        azure_y\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the results\n",
    "\n",
    "As you can see, the multiple linear regression model consistently predicts that Catalyst Cloud would offer either the cheapest, or competitively priced compute instances.\n",
    "\n",
    "While this is by no means a perfect indicator of who is the cheapest and should not be taken as such, it does serve to dispel the idea that Catalyst Cloud is overpriced, or cannot compete with international companies on price."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
